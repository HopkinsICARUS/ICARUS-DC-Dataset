
# Formatted Data Center Dataset

This directory contains the **formatted version** of the data center dataset.  
It represents the intermediate stage between raw cleaned data and fully imputed data.  
All structural, type, and categorical adjustments have been applied to prepare the dataset for statistical analysis and multiple imputation.

---

## 1. Overview

The formatted dataset standardizes and organizes the cleaned source data from:

```
data/data_center_dataset/
```


into a fully structured form suitable for model input, missing-data diagnostics, and transformation-based analysis.  
This version is not yet imputed — instead, it isolates all missingness explicitly for inspection and consistency checks.

---

## 2. Processing Steps

The following transformations are applied in this stage:

### **2.1 Schema Normalization**
- Ensures consistent column ordering and naming conventions:
  - Example: `"Power (MW)" → "power_mw"`, `"Whitespace (sqft)" → "whitespace_sqft"`.
- Converts all numerical columns to `float` or `int` types.
- Normalizes categorical capitalization and whitespace.

### **2.2 Date and Year Fields**
- Converts text-based year fields such as `"Year Built"` and `"Year Renovated"` to numeric form.
- Flags outlier or implausible values (e.g., future build dates) for later review.

### **2.3 Derived Metrics**
- Adds standardized computed features (e.g., `power_density = power_mw / whitespace_sqft` where possible).
- Maintains feature parity with downstream imputation to prevent schema drift.

### **2.4 Missingness Diagnostics**
- Identifies columns with >30% missingness.
- Records summary statistics for missingness patterns to guide imputation selection.
- Generates a null heatmap and summary report (optional step, via the data generation utilities).

### **2.5 Categorical Encoding**
- Prepares categorical columns for modeling:
  - Operator and Type fields are encoded using consistent integer or one-hot formats.
  - Non-numeric representations are standardized (e.g., `"Yes"/"No"` → `1/0`).

### **2.6 Outlier and Range Handling**
- Detects and, if configured, caps extreme numerical values using the IQR (interquartile range) method.
- Ensures that the final formatted dataset fits within reasonable physical and design constraints for data centers.

---

## 3. Link to Imputation Process

After formatting, the dataset is passed to the imputation pipeline (`data/imputed_data_center_dataset/`).

The imputation process, detailed in the **Imputation README**, replaces missing values using a multivariate model-based approach.  
In brief:

1. **Initialization:** All missing values are initially filled with mean estimates.  
2. **Iterative Modeling:** Each feature with missing data is predicted using all others in sequence.  
3. **Convergence:** The process repeats until updates stabilize or the maximum iteration limit is reached.  
4. **Finalization:** Numeric and categorical imputations are merged into a complete dataset.

The mathematical framework, including the use of *Iterative Imputer with Bayesian Ridge Regression*, is described fully in  
[`data/imputed_data_center_dataset/imputation_process.md`](../data/imputed_data_center_dataset/imputation_process.md).

---

## 4. Intended Use

The formatted dataset serves as the **canonical preprocessing layer** for all downstream analyses.  
It is designed to:
- Ensure full schema consistency across data sources.
- Preserve traceability between raw and imputed datasets.
- Allow modelers to inspect missingness before statistical imputation.

This version can be safely used for exploratory visualization, partial regression testing, or data completeness evaluation.

---

**Generated by:** `generate_all_data.py`  
**Upstream Source:** `data/data_center_dataset/`  
**Downstream Target:** `data/imputed_data_center_dataset/`

**Last Updated:** Automatically with each regeneration
